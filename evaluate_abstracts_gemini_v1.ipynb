{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Switch to Google's generative AI library\n",
    "from google.generativeai import models\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#load all abstracts\n",
    "df = pd.read_csv('all_abstracts_screen.csv')"
   ],
   "id": "d2811d7e6fbc1f9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "cf190a11d29e7e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to get the API key (adapt as needed for Gemini)\n",
    "def get_api_key():\n",
    "    # Hardcoded path to the secrets directory\n",
    "    secrets_dir = '/Users/chrisjanssen/Insync/cnjanssen@tamu.edu/Google Drive/COM/Research/spaceflightreview-secrets'\n",
    "\n",
    "    # Construct the full path to the secrets.txt file\n",
    "    secret_file = os.path.join(secrets_dir, 'gemini_secrets.txt')\n",
    "\n",
    "    # Read the API key from the file\n",
    "    try:\n",
    "        with open(secret_file, 'r') as file:\n",
    "            api_key = file.read().strip()\n",
    "        return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: secrets.txt file not found in the secrets directory: {secrets_dir}\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error: Unable to read the secrets.txt file: {secret_file}\")\n",
    "        return None\n",
    "\n",
    "# Get the Gemini API key\n",
    "gemini_api_key = get_api_key()\n",
    "\n",
    "if gemini_api_key:\n",
    "    print(\"API key successfully loaded.\")\n",
    "    # Correct way to configure the API key\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = gemini_api_key\n",
    "    print(f\"api key:{gemini_api_key}\")\n",
    "else:\n",
    "    print(\"Failed to load API key.\")\n",
    "\n"
   ],
   "id": "f81b925d1c81f54b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initial token count and limit (adjust based on Gemini's limits)\n",
    "running_token_count = 0\n",
    "TOKEN_LIMIT = 1000000  # Adjust as needed for Gemini Flash\n",
    "last_reset_time = time.time()\n",
    "import google.generativeai as genai\n",
    "\n",
    "#create model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "response = model.generate_content(\"The opposite of hot is\")\n",
    "#os.environ[\"GOOGLE_API_KEY\"] = gemini_api_key\n"
   ],
   "id": "8c3afb13c55f6f2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(response.text)",
   "id": "41b65df5029010b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Switch to Google's generative AI library\n",
    "from google.generativeai import models\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set up the API key for Google Gemini\n",
    "os.environ['GOOGLE_API_KEY'] = 'YOUR_GEMINI_API_KEY'  # Replace with your actual key\n",
    "\n",
    "# Cache the prompt\n",
    "@lru_cache(maxsize=1)\n",
    "def get_cached_prompt():\n",
    "    return \"\"\"You are assisting in screening articles for a scoping review on medical screening for Commercial Spaceflight (CSF). Your task is to determine whether to include, exclude, or mark as \"maybe\" each article based on the provided criteria. \n",
    "Here are the key inclusion criteria:\n",
    "- Timeframe: 2000-2023\n",
    "- Setting: Spaceflight experience/profile and/or analogues/simulation (e.g., suborbital, orbital, lunar, planetary, trans-atmospheric, parabolic, centrifuge, bed rest, dry immersion, head-down tilt)\n",
    "- Human medical conditions related to CSF\n",
    "- Language: English in full text\n",
    "- Sample: Adult human participants (18 years and older) but who are commercial spaceflight participants or passengers who DO NOT meet career professional astronaut medical standards and training and who DOES NOT HAVE primary duties with operation or safety of flgiht of aircraft\n",
    "- Peer-reviewed articles relevant to Commercial Spaceflight\n",
    "- NASA RIDGE hazards of space flight such as \n",
    "--Radiation\n",
    "--Isolation and confinement\n",
    "--Distance from Earth\n",
    "--Gravity (differing gravitational fields)\n",
    "--Environment (hostile/closed)\n",
    "\n",
    "Key exclusion criteria:\n",
    "- Animal, in-silico, or in-vitro studies\n",
    "- Pediatrics or age less than 18 years\n",
    "- Full text not available in English\n",
    "- Articles limited to professional astronauts or equivalent\n",
    "- Gray literature, dissertations, theses, technical reports, proprietary information\n",
    "- Editorials, magazine articles, or web-based/digital media\n",
    "- Textbooks\n",
    "\n",
    "Please review the following article information:\n",
    "\n",
    "Title: <article_title>{{Title}}</article_title>\n",
    "\n",
    "Year: <article_year>{{Published_Year}}</article_year>\n",
    "\n",
    "Abstract: <article_abstract>{{Abstract}}</article_abstract>\n",
    "\n",
    "Based on this information, determine whether the article should be included, excluded, or marked as \"maybe\" for the scoping review. Consider the inclusion and exclusion criteria carefully.\n",
    "\n",
    "First, provide a brief justification for your decision, explaining how the article meets or fails to meet the criteria. Then, state your decision.\n",
    "\n",
    "Format your response as follows:\n",
    "<justification>\n",
    "[Your justification here]\n",
    "</justification>\n",
    "\n",
    "<decision>[INCLUDE/EXCLUDE/MAYBE]</decision>\"\"\"\n",
    "\n"
   ],
   "id": "d48e9b359882dcf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to estimate tokens (simplified)\n",
    "def estimate_tokens(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "def process_article_with_retry(title, year, abstract, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.debug(f\"Attempting to process article (Attempt {attempt + 1})\")\n",
    "            return process_article(title, year, abstract)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                logging.warning(f\"Max retries reached. Skipping article.\")\n",
    "                return \"Error\", 0\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff"
   ],
   "id": "a5f6b1faef127895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_article(title, year, abstract):\n",
    "    global running_token_count, last_reset_time\n",
    "\n",
    "    # Handle potential None values\n",
    "    title = str(title) if pd.notna(title) else \"\"\n",
    "    year = str(year) if pd.notna(year) else \"\"\n",
    "    abstract = str(abstract) if pd.notna(abstract) else \"\"\n",
    "\n",
    "    prompt = get_cached_prompt().replace(\"{{Title}}\", title).replace(\"{{Published_Year}}\", year).replace(\"{{Abstract}}\", abstract)\n",
    "\n",
    "    tokens_used = estimate_tokens(prompt)\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check if a minute has passed since the last reset\n",
    "    if current_time - last_reset_time >= 60:\n",
    "        logging.debug(\"Resetting token count\")\n",
    "        running_token_count = 0\n",
    "        last_reset_time = current_time\n",
    "\n",
    "    # Check if adding these tokens would exceed the limit\n",
    "    if running_token_count + tokens_used > TOKEN_LIMIT:\n",
    "        sleep_time = 60 - (current_time - last_reset_time)\n",
    "        if sleep_time > 0:\n",
    "            logging.debug(f\"Sleeping for {sleep_time} seconds to avoid rate limit\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        # Reset token count after the sleep\n",
    "        running_token_count = 0\n",
    "        last_reset_time = time.time()\n",
    "\n",
    "    running_token_count += tokens_used\n",
    "\n",
    "    # Call the Gemini API\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0,\n",
    "                max_output_tokens=3000\n",
    "            )\n",
    "        )\n",
    "        print(response.text)\n",
    "        return response.text, tokens_used\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing article: {e}\")\n",
    "        return f\"Error: {str(e)}\", 0"
   ],
   "id": "3528b1ac9f4d8416",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process all articles in the dataframe\n",
    "def process_all_articles(df):\n",
    "    results = []\n",
    "    tokens_used_list = []\n",
    "    progress_bar = tqdm(total=len(df), desc=\"Processing Articles\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            logging.info(f\"Processing article {i+1}/{len(df)}\")\n",
    "            result, tokens_used = process_article_with_retry(row['Title'], row['Published Year'], row['Abstract'])\n",
    "            results.append(str(result))\n",
    "            tokens_used_list.append(tokens_used)\n",
    "            logging.info(f\"Successfully processed article {i+1}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing row {i}: {e}\")\n",
    "            results.append(\"Error\")\n",
    "            tokens_used_list.append(0)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            logging.info(f\"Checkpoint: Processed {i+1} articles\")\n",
    "            logging.debug(f\"Current results length: {len(results)}\")\n",
    "            logging.debug(f\"Current tokens_used_list length: {len(tokens_used_list)}\")\n",
    "            save_intermediate_results(df.iloc[:i+1], results, tokens_used_list, i+1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Add results to the DataFrame\n",
    "    df['AI_Decision'] = results\n",
    "    df['Tokens_Used'] = tokens_used_list\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_intermediate_results(df_slice, results, tokens_used_list, num_processed):\n",
    "    temp_df = df_slice.copy()\n",
    "    if len(results) != len(temp_df):\n",
    "        logging.warning(f\"Mismatch in lengths. DataFrame: {len(temp_df)}, Results: {len(results)}\")\n",
    "        # Pad results and tokens_used_list if necessary\n",
    "        results += [\"Error\"] * (len(temp_df) - len(results))\n",
    "        tokens_used_list += [0] * (len(temp_df) - len(tokens_used_list))\n",
    "    temp_df['AI_Decision'] = results[:len(temp_df)]\n",
    "    temp_df['Tokens_Used'] = tokens_used_list[:len(temp_df)]\n",
    "    temp_df.to_parquet(f'pt2_intermediate_results_{num_processed}.parquet')\n",
    "    logging.info(f\"Saved pt2 intermediate results for {num_processed} articles\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to plot progress\n",
    "def plot_progress(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Tokens_Used'].cumsum())\n",
    "    plt.title('Cumulative Tokens Used')\n",
    "    plt.xlabel('Article Index')\n",
    "    plt.ylabel('Total Tokens')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df['AI_Decision'].value_counts().plot(kind='bar')\n",
    "    plt.title('Distribution of AI Decisions')\n",
    "    plt.xlabel('Decision')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ],
   "id": "3f1df02ec5c421c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = process_all_articles(df)\n",
    "plot_progress(df)"
   ],
   "id": "6275803f693d5a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62ef6ea20723a394",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
